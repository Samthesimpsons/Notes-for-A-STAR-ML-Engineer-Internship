{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch\n",
    "\n",
    "Tensor is basically an MD array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-deaa343fdd1b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.Tensor([5,3])\n",
    "y = torch.Tensor([2,1])\n",
    "\n",
    "print(x*y)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 5])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.zeros([2,5])\n",
    "print(x)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 0., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 0., 1., 0.]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.eye(3,4)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4630, 0.4810, 0.1878, 0.7571, 0.2356],\n",
      "        [0.9163, 0.6695, 0.5935, 0.0710, 0.3576]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand([2,5])\n",
    "x.view([1,10]) #view flattens, reshapes the tensor\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even after \"reshaping\", flattening to 1x10 array, x has not beeen changed, we need to reassign\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4630, 0.4810, 0.1878, 0.7571, 0.2356, 0.9163, 0.6695, 0.5935, 0.0710,\n",
      "         0.3576]])\n"
     ]
    }
   ],
   "source": [
    "X = x.view([1,10])\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Our First Deep Model Network\n",
    "First, we need a dataset.\n",
    "\n",
    "We're just going to use data from Pytorch's \"torchvision.\" Pytorch has a relatively handy inclusion of a bunch of different datasets, including many for vision tasks, which is what torchvision is for.\n",
    "\n",
    "We're going to first start off by using Torchvision because you should know it exists, plus it alleviates us the headache of dealing with datasets from scratch.\n",
    "\n",
    "For now though, we're just trying to learn about how to do a basic neural network in pytorch, so we'll use torchvision here, to load the MNIST dataset, which is a image-based dataset showing handwritten digits from 0-9, and your job is to write a neural network to classify them.\n",
    "\n",
    "To begin, let's make our imports and load in the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets\n",
    "train = datasets.MNIST('',train=True,download=False,\n",
    "                      transform = transforms.Compose([transforms.ToTensor()]))\n",
    "test = datasets.MNIST('',train=False,download=False,\n",
    "                      transform = transforms.Compose([transforms.ToTensor()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, we're just loading in the dataset, shuffling it, and applying any transforms/pre-processing to a tensor object\n",
    "\n",
    "Next, we need to handle for how we're going to iterate over that dataset:\n",
    "\n",
    "## Training and Testing data split\n",
    "To train any machine learning model, we want to first off have training and validation datasets. This is so we can use data that the machine has never seen before to \"test\" the machine.\n",
    "\n",
    "## Shuffling\n",
    "Then, within our training dataset, we generally want to randomly shuffle the input data as much as possible to hopefully not have any patterns in the data that might throw the machine off.\n",
    "\n",
    "For example, if you fed the machine a bunch of images of zeros, the machine would learn to classify everything as zero. Then you'd start feeding it ones, and the machine would figure out pretty quick to classify everything as ones...and so on. Whenever you stop, the machine would probably just classify everything as the last thing you trained on. If you shuffle the data, your machine is much more likely to figure out what's what.\n",
    "\n",
    "## Scaling and normalization\n",
    "Another consideration at some point in the pipeline is usually scaling/normalization of the dataset. In general, we want all input data to be between zero and one. Often many datasets will contain data in ranges that are not within this range, and we generally will want to come up with a way to scale the data to be within this range.\n",
    "\n",
    "For example, an image is comprised of pixel values, most often in the range of 0 to 255. To scale image data, you usually just divide by 255. That's it.\n",
    "\n",
    "## Batches\n",
    "Once you've done all this, you then want to pass your training dataset to your neural network.\n",
    "\n",
    "Not so fast!\n",
    "\n",
    "There are two major reasons why you can't just go and pass your entire dataset at once to your neural network:\n",
    "\n",
    "Neural networks shine and outperform other machine learning techniques because of how well they work on big datasets. Gigabytes. Terabytes. Petabytes! \n",
    "\n",
    "When we're just learning, we tend to play with datasets smaller than a gigabyte, and we can often just toss the entire thing into the VRAM of our GPU or even more likely into RAM.\n",
    "\n",
    "Unfortunately, in practice, you would likely not get away with this.\n",
    "\n",
    "The aim with neural networks is to have the network generalize with the data. We want the neural network to actually learn general principles. That said, neural networks often have millions, or tens of millions, of parameters that they can tweak to do this. This means neural networks can also just memorize things. While we hope neural networks will generalize, they often learn to just memorize the input data. Our job as the scientist is to make it as hard as possible for the neural network to just memorize.\n",
    "This is another reason why we often track \"in sample\" validation acccuracy and \"out of sample\" validation accuracy. If these two numbers are similar, this is good. As they start to diverge (in sample usually goes up considerably while out of sample stays the same or drops), this usually means your neural network is starting to just memorize things.\n",
    "\n",
    "One way we can help the neural network to not memorize is, at any given time, we feed only some specific batch size of data. This is often something between 8 and 64.\n",
    "\n",
    "This batching helps because, with each batch, the neural network does a back propagation for new, updated weights with hopes of decreasing that loss.\n",
    "\n",
    "With one giant passing of your data, this would include neuron changes that had nothing to do with general principles and were just brute forcing the operation.\n",
    "\n",
    "By passing many batches, each with their own gradient calcs, loss, and backprop, this means each time the neural network optimizes things, it will sort of \"keep\" the changes that were actually useful, and erode the ones that were just basically memorizing the input data.\n",
    "\n",
    "Given a large enough neural network, however, even with batches, your network can still just simply memorize.\n",
    "\n",
    "This is also why we often try to make the smallest neural network as possible, so long as it still appears to be learning. In general, this will be a more successful model long term.\n",
    "\n",
    "Now what?\n",
    "\n",
    "Well, we have our data, but what is it really? How do we work with it? We can iterate over our data like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]]), tensor([7, 0, 6, 6, 9, 0, 2, 5, 3, 7])]\n"
     ]
    }
   ],
   "source": [
    "trainset = torch.utils.data.DataLoader(train, batch_size=10, shuffle=True)\n",
    "testset = torch.utils.data.DataLoader(test, batch_size=10, shuffle=True)\n",
    "\n",
    "for data in trainset:\n",
    "    print(data)\n",
    "    break\n",
    "    \n",
    "#iterating over the datasets, break to show one dataset of 10 digits\n",
    "#0th index is a tensor object containing a tensor of tensor which is the images\n",
    "#1st index is a tensor object which is the labels for the numbers themselves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us take a look at one:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.1765, 0.3255, 0.3255, 0.3255,\n",
      "          0.5961, 0.9294, 1.0000, 0.8549, 0.5216, 0.0902, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.1765, 0.6824, 0.9882, 0.9961, 0.8392, 0.5216,\n",
      "          0.3529, 0.3137, 0.4314, 0.9216, 0.9961, 0.6000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.2588, 0.9529, 0.9294, 0.5216, 0.0784, 0.0157, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.2000, 0.9216, 0.8196, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0745, 0.9333, 0.9961, 0.2471, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.3647, 0.7529, 0.9843, 0.9451, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.5882, 0.9961, 0.5725, 0.0078, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0745, 0.9059, 0.9725, 0.7412, 0.3608, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.9176, 0.8471, 0.0627, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.5098, 0.9961, 0.2902, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1647,\n",
      "          0.9725, 0.4392, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.3137, 0.9765, 0.9725, 0.1843, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2314,\n",
      "          0.9843, 0.5843, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.5412, 0.9961, 0.5922, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.8824, 0.9608, 0.2431, 0.0824, 0.0000, 0.0000, 0.1608, 0.6353,\n",
      "          0.9686, 0.9725, 0.2235, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.3020, 0.9059, 0.9961, 0.9098, 0.8471, 0.8471, 0.9725, 0.9961,\n",
      "          0.9961, 0.5294, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.1882, 0.6980, 0.8431, 0.8941, 0.8431, 0.7529, 0.9961,\n",
      "          0.9294, 0.1020, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0627, 0.0000, 0.4471, 0.9961,\n",
      "          0.5804, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0627, 0.7961, 0.9765,\n",
      "          0.2314, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.5569, 0.9961, 0.5412,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0196, 0.8745, 0.8941, 0.0980,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.4824, 0.9961, 0.4667, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0863, 0.9255, 0.7255, 0.0431, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.4745, 0.9961, 0.3451, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0941, 0.9333, 0.8471, 0.0431, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.5725, 0.9961, 0.4549, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000]]]) tensor(9)\n",
      "torch.Size([1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "X, y = data[0][4], data[1][4]\n",
    "print(X,y)\n",
    "print(data[0][0].shape)\n",
    "#take note the tensor object shape, has a 0th index of 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, data[1] is just a bunch of labels. So, since data[1][4] is a 9, we can expect data[0][4] to be an image of a 9. Let's see!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(data[0][4].view(28,28),cmap = 'gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## So, for our checklist:\n",
    "\n",
    "We've got our data of various featuresets and their respective classes.\n",
    "That data is all numerical.\n",
    "We've shuffled the data.\n",
    "We've split the data into training and testing groups.\n",
    "Is the data scaled?\n",
    "Is the data balanced?\n",
    "Looks like we have a couple more questions to answer. First off is it scaled? Remember earlier I warned that the neural network likes data to be scaled between 0 and 1 or -1 and 1. Raw imagery data is usually RGB, where each pixel is a tuple of values of 0-255, which is a problem. 0 to 255 is not scaled. How about our dataset here? Is it 0-255? or is it scaled already for us? Let's check out some lines:\n",
    "```python\n",
    "data[0][0][0][0] =\n",
    "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "        0., 0., 0., 0.])\n",
    "```\n",
    "\n",
    "Hmm, it's empty. Makes sense, the first few rows are blank probably in a lot of images. The 2 up above certainly is.\n",
    "\n",
    "```python\n",
    "data[0][0][0][3] =\n",
    "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "        0.0000, 0.0118, 0.4157, 0.9059, 0.9961, 0.9216, 0.5647, 0.1882, 0.0000,\n",
    "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "        0.0000])\n",
    "```\n",
    "\n",
    "Ah okay, there we go, we can clearly see that... yep this image data is actually already scaled for us.... in the real world, it wont be.\n",
    "\n",
    "Like I said: Cheating! Hah. Alright. One more question: Is the data balanced?\n",
    "\n",
    "## What is data balancing?\n",
    "\n",
    "Recall before how I explained that if we don't shuffle our data, the machine will learn things like what the last few hundred classes were in a row, and probably just predict that from there on out.\n",
    "\n",
    "Well, with data balancing, a similar thing could occur.\n",
    "\n",
    "Imagine you have a dataset of cats and dogs. 7200 images are dogs, and 1800 are cats. This is quite the imbalance. The classifier is highly likely to find out that it can very quickly and easily get to a 72% accuracy by simple always predicting dog. It is highly unlikely that the model will recover from something like this.\n",
    "\n",
    "Other times, the imbalance isn't quite as severe, but still enough to make the model almost always predict a certain way except in the most obvious-to-it-of cases. Anyway, it's best if we can balance the dataset.\n",
    "\n",
    "By \"balance,\" I mean make sure there are the same number of examples for each classifications in training.\n",
    "\n",
    "Sometimes, this simply isn't possible. There are ways for us to handle for this with special class weighting for the optimizer to take note of, but, even this doesn't always work. Personally, I've never had success with this in any real world application.\n",
    "\n",
    "In our case, how might we confirm the balance of data? Well, we just need to iterate over everything and make a count. Pretty simple:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 9.871666666666666\n",
      "1: 11.236666666666666\n",
      "2: 9.93\n",
      "3: 10.218333333333334\n",
      "4: 9.736666666666666\n",
      "5: 9.035\n",
      "6: 9.863333333333333\n",
      "7: 10.441666666666666\n",
      "8: 9.751666666666667\n",
      "9: 9.915000000000001\n",
      "{0: 5923, 1: 6742, 2: 5958, 3: 6131, 4: 5842, 5: 5421, 6: 5918, 7: 6265, 8: 5851, 9: 5949}\n"
     ]
    }
   ],
   "source": [
    "total = 0\n",
    "counter_dict = {0:0, 1:0, 2:0, 3:0, 4:0, 5:0, 6:0, 7:0, 8:0, 9:0}\n",
    "\n",
    "for data in trainset:\n",
    "    Xs, Ys = data\n",
    "    for labels in Ys:\n",
    "        counter_dict[int(labels)] += 1\n",
    "        total += 1\n",
    "        \n",
    "#percentage conversion\n",
    "for i in counter_dict:\n",
    "    print(f\"{i}: {counter_dict[i]/total*100}\")\n",
    "        \n",
    "print(counter_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a Nureal Network\n",
    "The torch.nn import gives us access to some helpful neural network things, such as various neural network layer types (things like regular fully-connected layers, convolutional layers (for imagery), recurrent layers...etc). For now, we've only spoken about fully-connected layers, so we will just be using those for now.\n",
    "\n",
    "The torch.nn.functional area specifically gives us access to some handy functions that we might not want to write ourselves. We will be using the relu or \"rectified linear\" activation function for our neurons. Instead of writing all of the code for these things, we can just import them, since these are things everyone will be needing in their deep learning code.\n",
    "\n",
    "To make our model, we're going to create a class. We'll call this class net and this net will inhereit from the nn.Module class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__() \n",
    "        #inorder to inherit the __init__ of the parent module, hence need super\n",
    "        #define the fully connected layers to the network\n",
    "        self.fc1 = nn.Linear(784, 64) \n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.fc3 = nn.Linear(64, 64)\n",
    "        self.fc4 = nn.Linear(64, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All we're doing is just defining values for some layers, we're calling them fc1, fc2...etc, but you could call them whatever you wanted. The fc just stands for fully connected. Fully connected refers to the point that every neuron in this layer is going to be fully connected to attaching neurons. Nothing fancy going on here! Recall, each \"connection\" comes with weights and possibly biases, so each connection is a \"parameter\" for the neural network to play with.\n",
    "\n",
    "In our case, we have 4 layers. Each of our nn.Linear layers expects the first parameter to be the input size, and the 2nd parameter is the output size.\n",
    "\n",
    "So, our first layer takes in 28x28, because our images are 28x28 images of hand-drawn digits. A basic neural network is going to expect to have a flattened array, so not a 28x28, but instead a 1x784.\n",
    "\n",
    "Then this outputs 64 connections. This means the next layer, fc2 takes in 64 (the next layer is always going to accept however many connections the previous layer outputs). From here, this layer ouputs 64, then fc3 just does the same thing.\n",
    "\n",
    "fc4 takes in 64, but outputs 10. Why 10? Our \"output\" layer needs 10 neurons for the 10 labels for each digit\n",
    "\n",
    "Now, that's great, we have those layers, but nothing really dictating how they interact with eachother, they're just simply defined.\n",
    "\n",
    "The simplest neural network is fully connected, and feed-forward, meaning we go from input to output. In one side and out the other in a \"forward\" manner. We do not have to do this, but, for this model, we will. So let's define a new method for this network called forward and then dictate how our data will pass through this model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__() \n",
    "        self.fc1 = nn.Linear(784, 64) \n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.fc3 = nn.Linear(64, 64)\n",
    "        self.fc4 = nn.Linear(64, 10)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.fc4(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the x is a parameter for the forward method. This will be our input data. As you can see, we literally just \"pass\" this data through the layers. This could in theory learn with some problems, but this is going to most likely cause some serious explosions in values. The neural network could control this, but probably wont. Instead, what we're missing is an activation function for the layers.\n",
    "\n",
    "Recall that we're mimicking brain neurons that either are firing, or not. We use activation functions to take the sum of the input data * weights, and then to determine if the neuron is firing or not.\n",
    "\n",
    "Currently, the most popular is the rectified linear, or relu, activation function.Basically, these activation functions are keeping our data scaled between 0 and 1.\n",
    "\n",
    "Finally, for the output layer, we're going to use softmax. Softmax makes sense to use for a multi-class problem, where each thing can only be one class or the other. This means the outputs themselves are a confidence score, adding up to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (fc1): Linear(in_features=784, out_features=64, bias=True)\n",
      "  (fc2): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (fc3): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (fc4): Linear(in_features=64, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__() \n",
    "        self.fc1 = nn.Linear(784, 64) \n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.fc3 = nn.Linear(64, 64)\n",
    "        self.fc4 = nn.Linear(64, 10)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x)) \n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.fc4(x) \n",
    "        #just know, in here, we can add logic actually, elif statements\n",
    "        \n",
    "        return F.log_softmax(x, dim=1) \n",
    "        #the output layer is always flat, hence the dim is 1 to apply softmax\n",
    "    \n",
    "net = Net()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, a fake random image to pass through our above network\n",
    "\n",
    "```python\n",
    "X = torch.rand((28,28))\n",
    "X = X.view(-1,28*28)\n",
    "output = net(X)\n",
    "print(output)\n",
    "\n",
    "tensor([[-2.2013, -2.2527, -2.3993, -2.1844, -2.2089, -2.4535, -2.4743, -2.2409,\n",
    "         -2.3953, -2.2698]], grad_fn=<LogSoftmaxBackward>)\n",
    "```\n",
    "\n",
    "Why the leading -1?\n",
    "Any input and output to our neural network is expected to be a group feature sets. Even if you intend to just pass 1 set of features, you still have to pass it as a \"list\" of features.\n",
    "\n",
    "In our case, we really just want a 1x784, and we could say that, but you will more often is -1 used in these shapings. Why? -1 suggests \"any size\". So it could be 1, 12, 92, 15295...etc. It's a handy way for that bit to be variable. In this case, the variable part is how many \"samples\" we'll pass through."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training our Neural NetworkÂ¶\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we want to calculate loss and specify our optimizer:\n",
    "\n",
    "Our loss_function is what calculates \"how far off\" our classifications are from reality. \n",
    "\n",
    "In terms of a machine learning by tweaking lots of little parameters to slowly get closer and closer to fitting, it definitely matters how wrong things are.\n",
    "\n",
    "For this, we use loss, which is a measurement of how far off the neural network is from the targeted output. There are a few types of loss calculations. A popular one is mean squared error, but we're trying to use these scalar-valued classes.\n",
    "\n",
    "In general, you're going to have two types of classes. One will just be a scalar value, the other is what's called a one_hot array/vector.\n",
    "\n",
    "In our case, a zero might be classified as:\n",
    "```python \n",
    "0 or [1, 0, 0, 0, 0, 0, 0 ,0 ,0 ,0]\n",
    "```\n",
    "The latter is a one_hot array where quite literally one element only is a 1 and the rest are zero. The index that is hot is the classification.\n",
    "\n",
    "A one_hot vector for a a 3 would be:\n",
    "```python\n",
    "[0, 0, 0, 1, 0, 0, 0 ,0 ,0 ,0]\n",
    "```\n",
    "I tend to use one_hot, but this data is specifying a scalar class, so 0, or 1, or 2...and so on.\n",
    "\n",
    "Depending on what your targets look like, you will need a specific loss.\n",
    "\n",
    "For one_hot vectors, I tend to use mean squared error.\n",
    "For these scalar classifications, I use cross entropy.\n",
    "\n",
    "Next, we have our optimizer. This is the thing that adjusts our model's adjustable parameters like the weights, to slowly, over time, fit our data. I am going to have us using Adam, which is Adaptive Momentum. This is the standard go-to optimizer usually. There's a new one called rectified adam that is gaining steam. I haven't had the chance yet to make use of that in any project, and I do not think it's available as just an importable function in Pytorch yet, but keep your eyes peeled for it! For now, Adam will do just fine I'm sure. \n",
    "\n",
    "The other thing here is lr, which is the learning rate. A good number to start with here is 0.001 or 1e-3. The learning rate dictates the magnitude of changes that the optimizer can make at a time. Thus, the larger the LR, the quicker the model can learn, but also you might find that the steps you allow the optimizer to make are actually too big and the optimizer gets stuck bouncing around rather than improving. Too small, and the model can take much longer to learn as well as also possibly getting stuck.\n",
    "\n",
    "Imagine the learning rate as the \"size of steps\" that the optimizer can take as it searches for the bottom of a mountain, where the path to the bottom isn't necessarily a simple straight path down.\n",
    "\n",
    "For simpler tasks, a learning rate of 0.001 usually is more than fine. For more complex tasks, you will see a learning rate with what's called a decay. Basically you start the learning rate at something like 0.001, or 0.01...etc, and then over time, that learning rate gets smaller and smaller. The idea being you can initially train fast, and slowly take smaller steps, hopefully getthing the best of both worlds:\n",
    "\n",
    "Now we can iterate over our data. In general, you will make more than just 1 pass through your entire training dataset.\n",
    "\n",
    "Each full pass through your dataset is referred to as an epoch. In general, you will probably have somewhere between 3 and 10 epochs, but there's no hard rule here.\n",
    "\n",
    "Too few epochs, and your model wont learn everything it could have.\n",
    "Too many epochs and your model will over fit to your in-sample data (basically memorize the in-sample data, and perform poorly on out of sample data).\n",
    "\n",
    "Let's go with 3 epochs for now. So we will loop over epochs, and each epoch will loop over our data. Something like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0083, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0095, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0289, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001) \n",
    "#net.parameters corresponds to everything that is adjustable in our model\n",
    "# 1st parameter is what you want to be adjusted by the model\n",
    "\n",
    "EPOCHS = 3\n",
    "\n",
    "for epoch in range(3): \n",
    "    for data in trainset:  \n",
    "        X, y = data  \n",
    "        net.zero_grad()  # sets gradients to 0 before loss calc. You will do this likely every step.\n",
    "        output = net(X.view(-1,784))  # pass in the reshaped batch \n",
    "        loss = F.nll_loss(output, y)  # calc and grab the loss value\n",
    "        loss.backward()  # apply this loss backwards thru the network's parameters\n",
    "        optimizer.step()  # attempt to optimize weights to account for loss/gradients\n",
    "    print(loss) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every line here is commented, but the concept of gradients might not be clear. Once we pass data through our neural network, getting an output, we can compare that output to the desired output. With this, we can compute the gradients for each parameter, which our optimizer (Adam, SGD...etc) uses as information for updating weights.\n",
    "\n",
    "This is why it's important to do a net.zero_grad() for every step, otherwise these gradients will add up for every pass, and then we'll be re-optimizing for previous gradients that we already optimized for. There could be times when you intend to have the gradients sum per pass, like maybe you have a batch of 10, but you want to optimize per 50 or something. I don't think people really do that, but the idea of Pytorch is to let you do whatever you want.\n",
    "\n",
    "So, for each epoch, and for each batch in our dataset, what do we do?\n",
    "\n",
    "Grab the features (X) and labels (y) from current batch\n",
    "Zero the gradients (net.zero_grad)\n",
    "Pass the data through the network\n",
    "Calculate the loss\n",
    "Adjust weights in the network with the hopes of decreasing loss\n",
    "As we iterate, we get loss, which is an important metric, but we care about accuracy. So, how did we do? To test this, all we need to do is iterate over our test set, measuring for correctness by comparing output to target values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.979\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in trainset:\n",
    "        X, y = data\n",
    "        output = net(X.view(-1,784))\n",
    "        #our model output is an argmax, our target is a scalar value\n",
    "        for idx, j in enumerate(output):\n",
    "            if torch.argmax(j) == y[idx]:\n",
    "                correct += 1\n",
    "            total += 1\n",
    "print('Accuracy:', round(correct/total,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets visualize the comparison\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.5413e+01, -9.9509e+00, -8.3054e+00, -7.2985e+00, -1.2345e+01,\n",
      "         -1.6147e+01, -2.7814e+01, -1.0405e-03, -1.2696e+01, -9.7076e+00]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "tensor(7, grad_fn=<NotImplemented>)\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(X[4].view(28,28))\n",
    "plt.show()\n",
    "\n",
    "print(net(X[4].view(-1,784)))\n",
    "print(torch.argmax(net(X[4].view(-1,784))[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convulsional Network\n",
    "Convulsional -> Pooling -> Basically it is simplifying the image by looking for features of the image -> layering them until it is a basic feature\n",
    "\n",
    "Now that we've learned about the basic feed forward, fully connected, neural network, it's time to cover a new one: the convolutional neural network, often referred to as a convnet or cnn.\n",
    "\n",
    "For the purposes of this tutorial, assume each square is a pixel. Next, for the convolution step, we're going to take some n-sized window:\n",
    "\n",
    "Those features are then condensed down into a single feature in a new featuremap.\n",
    "\n",
    "Next, we slide that window over and repeat until with have a new set of featuremaps.\n",
    "\n",
    "You continue this process until you've covered the entire image.\n",
    "From here, we do pooling. Now we'll take a 3x3 pooling window:\n",
    "The most common form of pooling is \"max pooling,\" where we simple take the maximum value in the window, and that becomes the new value for that region.\n",
    "\n",
    "Often, after convolutional layers, we'll have 1 or a few fully connected layers, and then the output. You could go straight from the final pooling to an output layer, however.\n",
    "\n",
    "So, at their core, convnets are hunting first for low level types of features like lines and curves, then they look for combinations of those, and so on, to better understand what they're looking at.\n",
    "\n",
    "Let's try our hands at a convnet example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing data\n",
    "To begin, we need a dataset.\n",
    "I am going to have us use the Cats vs Dogs dataset.\n",
    "\n",
    "This dataset consists of a bunch of images of cats and dogs. Different breeds, ages, sizes (both the animal and the image)..etc.\n",
    "\n",
    "Once you have downloaded the dataset, you need to extract it. I would just extract it to the directory that you're working in\n",
    "\n",
    "Remember how before I said using torchvision was cheating? Well it was, and now we have to build this data ourselves! To begin, let's make sure you have all of the required libraries.\n",
    "\n",
    "The IMG_SIZE is whatever we want, but we have to pick something. The images in the training data are all varying sizes and shapes. We're going to normalize all of the images by reshaping them to all be the same size. I will go with 50x50.\n",
    "\n",
    "Next are just some variables that hold where the directories with the data are. Once extracted, you wind up with 2 directories. One is Cat, the other is Dog and those contain a bunch of images.\n",
    "\n",
    "We want to iterate through these two directories, grab the images, resize, scale, convert the class to number (cats = 0, dogs = 1), and add them to our training_data.\n",
    "\n",
    "Continuing along in our class, let's make a new method called make_training_data:\n",
    "```python\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "class DogsVSCats():\n",
    "    IMG_SIZE = 50\n",
    "    CATS = \"PetImages/Cat\"\n",
    "    DOGS = \"PetImages/Dog\"\n",
    "    TESTING = \"PetImages/Testing\"\n",
    "    LABELS = {CATS: 0, DOGS: 1}\n",
    "    training_data = []\n",
    "    \n",
    "    def make_training_data(self):\n",
    "        for label in self.LABELS:\n",
    "        #iterating over the classvariable dictionary keys, which are directory of the main folder to the images Cats & Dogs\n",
    "            for file in tqdm(os.listdir(label)):\n",
    "            #now the iterating the images in the directory, file is the image file name itself, os.listdir returns all files\n",
    "                if 'jpg' in file:\n",
    "                    try:\n",
    "                        pass\n",
    "                    except Exception as e:\n",
    "                        pass\n",
    "                        #print(label, f, str(e))                    \n",
    "                    ```\n",
    "                    \n",
    "All we're doing so far is iterating through the cats and dogs directories, and looking through all of the images. Now let's actually write the code to handle for the images.\n",
    "\n",
    "We read in the data, convert to grayscale, resize the image to whatever we chose, and then append the image data along with the associated class in number form to our training_data. Some images can't be read, so we just pass on the exception. If you're creating no data at all, then go ahead and print out the error, but it's enough images to be annoying if I print out the error every time.\n",
    "\n",
    "Once we have the data, is there anything we've not done to the data yet?\n",
    "\n",
    "We want to check for balance, and we want to shuffle it. We can just use a counter again to see balance:\n",
    "```python\n",
    "                        if label == self.CATS:\n",
    "                            self.catcount += 1\n",
    "                        elif label == self.DOGS:\n",
    "                            self.dogcount += 1\n",
    "                        ```\n",
    "Then we can just shuffle the training_data list at the end with\n",
    "np.random.shuffle(self.training_data)\n",
    "\n",
    "This process can take a while, longer than we'd like if we're just tinkering with different values for our neural network. It'd be nice to just save where we are now after pre-processing, so we'll also add a np.save.\n",
    "\n",
    "Finally, I would just recommend using some sort of flag or something for if/when you change something like image shape or something like that, so you can easily re-run this code when needed.\n",
    "\n",
    "```python\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tqdm import tqdm                        \n",
    "                        \n",
    "REBUILD_DATA = False \n",
    "# set to true to one once, then back to false unless you want to change something in your training data.\n",
    "\n",
    "class DogsVSCats():\n",
    "    IMG_SIZE = 50\n",
    "    CATS = \"PetImages/Cat\"\n",
    "    DOGS = \"PetImages/Dog\"\n",
    "    TESTING = \"PetImages/Testing\"\n",
    "    LABELS = {CATS: 0, DOGS: 1}\n",
    "    training_data = []\n",
    "\n",
    "    catcount = 0\n",
    "    dogcount = 0\n",
    "\n",
    "    def make_training_data(self):\n",
    "        for label in self.LABELS:\n",
    "            print(label)\n",
    "            for f in tqdm(os.listdir(label)):\n",
    "                if \"jpg\" in f:\n",
    "                    try:\n",
    "                        path = os.path.join(label, f)\n",
    "                        #full path to the image\n",
    "                        \n",
    "                        img = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n",
    "                        #now we load the image, we convert to grayscale\n",
    "                        \n",
    "                        img = cv2.resize(img, (self.IMG_SIZE, self.IMG_SIZE))\n",
    "                        #now we resize the image\n",
    "                        \n",
    "                        self.training_data.append([np.array(img), np.eye(2)[self.LABELS[label]]])  \n",
    "                        # do something like print(np.eye(2)[1]), just makes one_hot vector \n",
    "                        # print(np.eye(2)[self.LABELS[label]])\n",
    "                        \n",
    "                        if label == self.CATS:\n",
    "                            self.catcount += 1\n",
    "                        elif label == self.DOGS:\n",
    "                            self.dogcount += 1\n",
    "\n",
    "                    except Exception as e:\n",
    "                        pass\n",
    "                        #print(label, f, str(e))\n",
    "\n",
    "        np.random.shuffle(self.training_data)\n",
    "        np.save(\"training_data.npy\", self.training_data)\n",
    "        #saves the training_data as file name\n",
    "        \n",
    "        print('Cats:',dogsvcats.catcount)\n",
    "        print('Dogs:',dogsvcats.dogcount)\n",
    "\n",
    "if REBUILD_DATA: #so if True, run the following classes events\n",
    "    dogsvcats = DogsVSCats()\n",
    "    dogsvcats.make_training_data()\n",
    "    ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24946\n"
     ]
    }
   ],
   "source": [
    "#Now we have saved the built data, now to use it, lets bring it back in\n",
    "training_data = np.load(\"training_data.npy\", allow_pickle=True)\n",
    "print(len(training_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 0.])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "X = torch.Tensor([i[0] for i in training_data]).view(-1,50,50)\n",
    "#convert numpy to tensor\n",
    "X = X/255.0\n",
    "#normalizing the pixel values\n",
    "y = torch.Tensor([i[1] for i in training_data])\n",
    "\n",
    "#One image example\n",
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(X[0], cmap=\"gray\")\n",
    "print(y[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright! So now we have our training data in the form of inputs and outputs.\n",
    "\n",
    "Now, we're going to build the CNN. We'll begin with some basic imports:\n",
    "Next, we'll make a Net class again, this time having the layers be convolutional.\n",
    "\n",
    "The layers have 1 more parameter after the input and output size, which is the kernel window size. This is the size of the \"window\" that you take of pixels. A 5 means we're doing a sliding 5x5 window for colvolutions.\n",
    "\n",
    "The same rules apply, where you see the first layer takes in 1 image, outputs 32 convolutions, then the next is going to take in 32 convolutions/features, and output 64 more...and so on.\n",
    "\n",
    "Now comes a new concept. Convolutional features are just that, they're convolutions, maybe max-pooled convolutions, but they aren't flat. We need to flatten them, like we need to flatten an image before passing it through a regular layer.\n",
    "\n",
    "Initially image size, WxH = 50x50\n",
    "Kernel Size, k = 5\n",
    "Stride , s = 1\n",
    "Padding, P = 0\n",
    "\n",
    "No. of outputs/pixels to next layer of conv2d is O = {(W-k+2*P)/s} + 1\n",
    "Then we apply maxpool of size(2,2) which reduces the size by half across both dimensions of the image, hence new output size O = O/2\n",
    "\n",
    "At the last conv2d later, the input size to the linear layer is the image volume flattened out, hence it would be last O*O*last channel width\n",
    "\n",
    "The input image is 50 * 50, so:\n",
    "O1 = (50-5+2*0)/1 + 1=46\n",
    "O1 max pooling = (46)/2 = 23\n",
    "O2 = (23-5+2*0)/1 + 1 = 19\n",
    "O2 max pooling = (19)/2 = 8.5, crops out the half pixel, hence = 8\n",
    "O3 = (8-5+2*0)/1 + 1 = 4\n",
    "O3 max pooling = 4/2 = 2\n",
    "\n",
    "Last volume is 2* 2*Channelwidthoutput,128 = 512\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv3): Conv2d(64, 128, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (fc2): Linear(in_features=512, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__() # just run the init of parent class (nn.Module)\n",
    "        self.conv1 = nn.Conv2d(1, 32, 5) # input is 1 image, 32 output channels, 5x5 kernel / window\n",
    "        self.conv2 = nn.Conv2d(32, 64, 5) # input is 32, then we say the output will be 64 channels, 5x5 kernel / window\n",
    "        self.conv3 = nn.Conv2d(64, 128, 5)\n",
    "        self.fc1 = nn.Linear(2*2*128, 512) #flattening\n",
    "        self.fc2 = nn.Linear(512, 2) # 512 in, 2 out bc we're doing 2 classes (dog vs cat).\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), (2, 2))\n",
    "        x = F.max_pool2d(F.relu(self.conv3(x)), (2, 2))\n",
    "        x = x.view(-1, 512)  # .view is reshape, this flattens X before \n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x) # bc this is our output layer. No activation here.\n",
    "        return F.softmax(x, dim=1) \n",
    "\n",
    "net = Net()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.Adam(net.parameters(), lr = 0.001)\n",
    "loss_function = nn.MSELoss() \n",
    "\n",
    "X = torch.Tensor([i[0] for i in training_data]).view(-1,50,50)\n",
    "X = X/255.0 \n",
    "y = torch.Tensor([i[1] for i in training_data])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, we're separating out the featuresets (X) and labels (y) from the training data. Then, we're viewing the X data as (-1, 50, 50), where the 50 is coming from image size. Now, we want to separate out some of the data for validation/out of sample testing.\n",
    "\n",
    "To do this, let's just say we want to use 10% of the data for testing. We can achieve this by doing:\n",
    "We're converting to an int because we're going to use this number to slice our data into groups, so it needs to be a valid index:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2494\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(22452, 2494)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VAL_PCT = 0.1  # lets reserve last 10% of our data for validation\n",
    "val_size = int(len(X)*VAL_PCT)\n",
    "print(val_size)\n",
    "\n",
    "train_X = X[:-val_size]\n",
    "train_y = y[:-val_size]\n",
    "\n",
    "test_X = X[-val_size:]\n",
    "test_y = y[-val_size:]\n",
    "\n",
    "len(train_X), len(test_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we want to actually iterate over this data to fit and test. We need to decide on a batch size. If you get any memory errors, go ahead and lower the batch size. I am going to go with 100 for now:\n",
    "\n",
    "Then lets print a str to see visualize our slicing of the batches of data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:100\n",
      "100:200\n",
      "200:300\n",
      "300:400\n",
      "400:500\n",
      "500:600\n",
      "600:700\n",
      "700:800\n",
      "800:900\n",
      "900:1000\n",
      "1000:1100\n",
      "1100:1200\n",
      "1200:1300\n",
      "1300:1400\n",
      "1400:1500\n",
      "1500:1600\n",
      "1600:1700\n",
      "1700:1800\n",
      "1800:1900\n",
      "1900:2000\n",
      "2000:2100\n",
      "2100:2200\n",
      "2200:2300\n",
      "2300:2400\n",
      "2400:2500\n",
      "2500:2600\n",
      "2600:2700\n",
      "2700:2800\n",
      "2800:2900\n",
      "2900:3000\n",
      "3000:3100\n",
      "3100:3200\n",
      "3200:3300\n",
      "3300:3400\n",
      "3400:3500\n",
      "3500:3600\n",
      "3600:3700\n",
      "3700:3800\n",
      "3800:3900\n",
      "3900:4000\n",
      "4000:4100\n",
      "4100:4200\n",
      "4200:4300\n",
      "4300:4400\n",
      "4400:4500\n",
      "4500:4600\n",
      "4600:4700\n",
      "4700:4800\n",
      "4800:4900\n",
      "4900:5000\n",
      "5000:5100\n",
      "5100:5200\n",
      "5200:5300\n",
      "5300:5400\n",
      "5400:5500\n",
      "5500:5600\n",
      "5600:5700\n",
      "5700:5800\n",
      "5800:5900\n",
      "5900:6000\n",
      "6000:6100\n",
      "6100:6200\n",
      "6200:6300\n",
      "6300:6400\n",
      "6400:6500\n",
      "6500:6600\n",
      "6600:6700\n",
      "6700:6800\n",
      "6800:6900\n",
      "6900:7000\n",
      "7000:7100\n",
      "7100:7200\n",
      "7200:7300\n",
      "7300:7400\n",
      "7400:7500\n",
      "7500:7600\n",
      "7600:7700\n",
      "7700:7800\n",
      "7800:7900\n",
      "7900:8000\n",
      "8000:8100\n",
      "8100:8200\n",
      "8200:8300\n",
      "8300:8400\n",
      "8400:8500\n",
      "8500:8600\n",
      "8600:8700\n",
      "8700:8800\n",
      "8800:8900\n",
      "8900:9000\n",
      "9000:9100\n",
      "9100:9200\n",
      "9200:9300\n",
      "9300:9400\n",
      "9400:9500\n",
      "9500:9600\n",
      "9600:9700\n",
      "9700:9800\n",
      "9800:9900\n",
      "9900:10000\n",
      "10000:10100\n",
      "10100:10200\n",
      "10200:10300\n",
      "10300:10400\n",
      "10400:10500\n",
      "10500:10600\n",
      "10600:10700\n",
      "10700:10800\n",
      "10800:10900\n",
      "10900:11000\n",
      "11000:11100\n",
      "11100:11200\n",
      "11200:11300\n",
      "11300:11400\n",
      "11400:11500\n",
      "11500:11600\n",
      "11600:11700\n",
      "11700:11800\n",
      "11800:11900\n",
      "11900:12000\n",
      "12000:12100\n",
      "12100:12200\n",
      "12200:12300\n",
      "12300:12400\n",
      "12400:12500\n",
      "12500:12600\n",
      "12600:12700\n",
      "12700:12800\n",
      "12800:12900\n",
      "12900:13000\n",
      "13000:13100\n",
      "13100:13200\n",
      "13200:13300\n",
      "13300:13400\n",
      "13400:13500\n",
      "13500:13600\n",
      "13600:13700\n",
      "13700:13800\n",
      "13800:13900\n",
      "13900:14000\n",
      "14000:14100\n",
      "14100:14200\n",
      "14200:14300\n",
      "14300:14400\n",
      "14400:14500\n",
      "14500:14600\n",
      "14600:14700\n",
      "14700:14800\n",
      "14800:14900\n",
      "14900:15000\n",
      "15000:15100\n",
      "15100:15200\n",
      "15200:15300\n",
      "15300:15400\n",
      "15400:15500\n",
      "15500:15600\n",
      "15600:15700\n",
      "15700:15800\n",
      "15800:15900\n",
      "15900:16000\n",
      "16000:16100\n",
      "16100:16200\n",
      "16200:16300\n",
      "16300:16400\n",
      "16400:16500\n",
      "16500:16600\n",
      "16600:16700\n",
      "16700:16800\n",
      "16800:16900\n",
      "16900:17000\n",
      "17000:17100\n",
      "17100:17200\n",
      "17200:17300\n",
      "17300:17400\n",
      "17400:17500\n",
      "17500:17600\n",
      "17600:17700\n",
      "17700:17800\n",
      "17800:17900\n",
      "17900:18000\n",
      "18000:18100\n",
      "18100:18200\n",
      "18200:18300\n",
      "18300:18400\n",
      "18400:18500\n",
      "18500:18600\n",
      "18600:18700\n",
      "18700:18800\n",
      "18800:18900\n",
      "18900:19000\n",
      "19000:19100\n",
      "19100:19200\n",
      "19200:19300\n",
      "19300:19400\n",
      "19400:19500\n",
      "19500:19600\n",
      "19600:19700\n",
      "19700:19800\n",
      "19800:19900\n",
      "19900:20000\n",
      "20000:20100\n",
      "20100:20200\n",
      "20200:20300\n",
      "20300:20400\n",
      "20400:20500\n",
      "20500:20600\n",
      "20600:20700\n",
      "20700:20800\n",
      "20800:20900\n",
      "20900:21000\n",
      "21000:21100\n",
      "21100:21200\n",
      "21200:21300\n",
      "21300:21400\n",
      "21400:21500\n",
      "21500:21600\n",
      "21600:21700\n",
      "21700:21800\n",
      "21800:21900\n",
      "21900:22000\n",
      "22000:22100\n",
      "22100:22200\n",
      "22200:22300\n",
      "22300:22400\n",
      "22400:22500\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 100 \n",
    "EPOCHS = 1\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    for i in range(0, len(train_X), BATCH_SIZE): \n",
    "    #so we start from 0 to the end of the len of the data set, and the step size is the batch size\n",
    "        print(f\"{i}:{i+BATCH_SIZE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|ââââââââââ| 225/225 [01:06<00:00,  3.36it/s]\n",
      "  0%|          | 0/225 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 0.21088474988937378\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|ââââââââââ| 225/225 [01:04<00:00,  3.47it/s]\n",
      "  0%|          | 0/225 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Loss: 0.1450960785150528\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|ââââââââââ| 225/225 [01:05<00:00,  3.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Loss: 0.13346624374389648\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 100\n",
    "EPOCHS = 3\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    for i in tqdm(range(0, len(train_X), BATCH_SIZE)): \n",
    "        batch_X = train_X[i:i+BATCH_SIZE].view(-1, 1, 50, 50) #then, reshape too\n",
    "        batch_y = train_y[i:i+BATCH_SIZE]\n",
    "        \n",
    "        #as usual first 0 the gradient for each batch\n",
    "        net.zero_grad()\n",
    "        \n",
    "        #optimizer.zero_grad()\n",
    "        #take note in some nn, there may be 2> same optimizers, so we might want to be apply 0_grad to specific optimizer\n",
    "        #or to the whole newtwork\n",
    "        \n",
    "        outputs = net(batch_X)\n",
    "        #gives us our output after running through our CNN\n",
    "        \n",
    "        loss = loss_function(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        #loss function applied, then back propagate\n",
    "        \n",
    "        optimizer.step()\n",
    "        #does the update\n",
    "\n",
    "    print(f\"Epoch: {epoch}, Loss: {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|ââââââââââ| 2494/2494 [00:04<00:00, 505.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.776\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for i in tqdm(range(len(test_X))):\n",
    "        real_class = torch.argmax(test_y[i]) #return the index of the testing set y label\n",
    "        \n",
    "        net_out = net(test_X[i].view(-1, 1, 50, 50))[0]  \n",
    "        predicted_class = torch.argmax(net_out)\n",
    "        \n",
    "        if predicted_class == real_class:\n",
    "            correct += 1\n",
    "        total += 1\n",
    "print(\"Accuracy:\", round(correct/total, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After 3 EPOCHes, running the data throught the network, the accuracy is 0.776. That said, as we continue to do testing and learn new things about how to measure performance of models, when to stop training...etc, it's going to be muuuuuuch more comfortable if we're using GPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're ready to decide what we want to do on the GPU. We know at the very least we want our model and its calculations to be done on the GPU. If your model is on the GPU, this means, in order to pass data through it, we also want our data on the GPU. Thus, we want not only the model, but also the training data (if it can be fit), all on the GPU.\n",
    "\n",
    "To start, we can put our network on our GPU. To do this, we can just set a flag like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Often, however, we want to write code that allows for a variety of people to use our code, including those who may not have a GPU available. To handle for this, we can use the above torch.cuda.is_available() and do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on the GPU\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")  \n",
    "    # you can continue going on here, like cuda:1 cuda:2....etc. \n",
    "    print(\"Running on the GPU\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Running on the CPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, to write code that can logically use what's available to dispense the workload across the numerous GPUs, you can get how many GPUs are available by doing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From here, we could extrapolate out index numbers and assign specific layers to specific GPUs.\n",
    "\n",
    "For now, we're writing code that really just needs either one GPU or CPU, so we'll just use a single device. Now that we have figured out the best device to use, we can start setting things to that device. For example, setting our neural network to that device is as easy as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (conv1): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (conv2): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (conv3): Conv2d(64, 128, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (fc1): Linear(in_features=512, out_features=512, bias=True)\n",
       "  (fc2): Linear(in_features=512, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We alread had our net defined above, but, usually you'd just immediately define and send it to the device, like:\n",
    "\n",
    "```python\n",
    "net = Net().to(device)\n",
    "```\n",
    "Now we can go to train, but this time, let's put our batches on the GPU. In this example, we could actually have put all of our data on the GPU, since it's not a huge dataset. This would save with some IO time moving things from RAM to VRAM, But this wont be a very common, so I'd rather just show the way you're going to normally have to most likely do it.\n",
    "\n",
    "I am going to copy the above train function and then make really just one quick modification, which is, after we've defined our batches, we can move them to the GPU by doing: batch_X, batch_y = batch_X.to(device), batch_y.to(device). Also we need to shift the optimizer and the batch_size too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|ââââââââââ| 225/225 [01:21<00:00,  2.77it/s] \n",
      "  3%|â         | 6/225 [00:00<00:03, 58.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0. Loss: 0.11765135824680328\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|ââââââââââ| 225/225 [00:04<00:00, 53.97it/s]\n",
      "  3%|â         | 6/225 [00:00<00:03, 56.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1. Loss: 0.11332621425390244\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|ââââââââââ| 225/225 [00:04<00:00, 51.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2. Loss: 0.0875396579504013\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 3\n",
    "\n",
    "def train(net):\n",
    "    optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "    BATCH_SIZE = 100\n",
    "    for epoch in range(EPOCHS):\n",
    "        for i in tqdm(range(0, len(train_X), BATCH_SIZE)):\n",
    "            batch_X = train_X[i:i+BATCH_SIZE].view(-1, 1, 50, 50)\n",
    "            batch_y = train_y[i:i+BATCH_SIZE]\n",
    "\n",
    "            batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "            \n",
    "            net.zero_grad()\n",
    "\n",
    "            optimizer.zero_grad()  \n",
    "            outputs = net(batch_X)\n",
    "            loss = loss_function(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()  \n",
    "\n",
    "        print(f\"Epoch: {epoch}. Loss: {loss}\")\n",
    "\n",
    "train(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see by running times, this is much faster. Now we can also test on either the GPU or CPU. Since we're testing on quite a few samples, we can also do this on the GPU:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|ââââââââââ| 2494/2494 [00:05<00:00, 476.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.78\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def test(net):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for i in tqdm(range(len(test_X))):\n",
    "            real_class = torch.argmax(test_y[i]).to(device)\n",
    "            \n",
    "            net_out = net(test_X[i].view(-1, 1, 50, 50).to(device))[0]  \n",
    "            predicted_class = torch.argmax(net_out)\n",
    "\n",
    "            if predicted_class == real_class:\n",
    "                correct += 1\n",
    "            total += 1\n",
    "\n",
    "    print(\"Accuracy: \", round(correct/total, 3))\n",
    "\n",
    "test(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Network Analysis and Visualizations\n",
    "One of the main questions that we have at the moment is: How many epochs should we do? When will we know when to stop training?\n",
    "\n",
    "We've determined that this model does learn, but also, how might we determine if some other model that we think of is better or worse?\n",
    "\n",
    "With larger datasets and models that can take days/weeks/months to train, we want to know how to evaluate them in shorter periods of time to determine if we should keep training.\n",
    "\n",
    "To begin, we'll, at the very least, want to start calculating accuracy, and loss, at the epoch (or even more granular) level.\n",
    "\n",
    "Not only this, but we'll want to calculate two accuracies:\n",
    "\n",
    "In-sample accuracy: This is the accuracy on the data we're actually feeding through the model for training. This is the data that we're \"fitting\" against.\n",
    "\n",
    "Out-of-sample accuracy: This is the accuracy on data that we've set aside that the model will never see/fit against.\n",
    "\n",
    "In general, we expect in-sample accuracy to be higher than out-of-sample accuracy. You may also hear \"out of sample\" accuracy referred to as \"validation accuracy.\" While we expect validation/out-of-sample accuracy to be a bit worse than the in-sample accuracy, we want to track that delta. Something like 5-10% accuracy difference is pretty common, but as this delta grows, it usually signals to us that our model is beginning to \"overfit\" (the neural network is just memorizing the data and changing weights to work only for the training data, rather than generally understanding the data).\n",
    "\n",
    "You can also track in and out of sample loss. You will often be able to spot both losses decline to a point, and then out of sample loss begins to arc and curve back upwards. This is usually a sign that you've gotten the most out of your model's training.\n",
    "\n",
    "We can see this in practice with our current dataset most likely, so let's start to work on our code to handle for this. Our code up to this point is:\n",
    "\n",
    "```python\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "REBUILD_DATA = False # set to true to one once, then back to false unless you want to change something in your training data.\n",
    "\n",
    "class DogsVSCats():\n",
    "    IMG_SIZE = 50\n",
    "    CATS = \"PetImages/Cat\"\n",
    "    DOGS = \"PetImages/Dog\"\n",
    "    TESTING = \"PetImages/Testing\"\n",
    "    LABELS = {CATS: 0, DOGS: 1}\n",
    "    training_data = []\n",
    "\n",
    "    catcount = 0\n",
    "    dogcount = 0\n",
    "\n",
    "    def make_training_data(self):\n",
    "        for label in self.LABELS:\n",
    "            print(label)\n",
    "            for f in tqdm(os.listdir(label)):\n",
    "                if \"jpg\" in f:\n",
    "                    try:\n",
    "                        path = os.path.join(label, f)\n",
    "                        img = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n",
    "                        img = cv2.resize(img, (self.IMG_SIZE, self.IMG_SIZE))\n",
    "                        self.training_data.append([np.array(img), np.eye(2)[self.LABELS[label]]]) \n",
    "\n",
    "                        if label == self.CATS:\n",
    "                            self.catcount += 1\n",
    "                        elif label == self.DOGS:\n",
    "                            self.dogcount += 1\n",
    "                    except Exception as e:\n",
    "                        pass\n",
    "\n",
    "        np.random.shuffle(self.training_data)\n",
    "        np.save(\"training_data.npy\", self.training_data)\n",
    "        print('Cats:',dogsvcats.catcount)\n",
    "        print('Dogs:',dogsvcats.dogcount)\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__() \n",
    "        self.conv1 = nn.Conv2d(1, 32, 5) \n",
    "        self.conv2 = nn.Conv2d(32, 64, 5) \n",
    "        self.conv3 = nn.Conv2d(64, 128, 5)\n",
    "        self.fc1 = nn.Linear(2*2*128, 512) \n",
    "        self.fc2 = nn.Linear(512, 2) \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), (2, 2))\n",
    "        x = F.max_pool2d(F.relu(self.conv3(x)), (2, 2))\n",
    "        x = x.view(-1, 512)  \n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x) \n",
    "        return F.softmax(x, dim=1) \n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\") \n",
    "    print(\"Running on the GPU\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Running on the CPU\")\n",
    "\n",
    "net = Net().to(device)\n",
    "\n",
    "if REBUILD_DATA:\n",
    "    dogsvcats = DogsVSCats()\n",
    "    dogsvcats.make_training_data()\n",
    "\n",
    "training_data = np.load(\"training_data.npy\", allow_pickle=True)\n",
    "\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "loss_function = nn.MSELoss()\n",
    "\n",
    "X = torch.Tensor([i[0] for i in training_data]).view(-1, 50, 50)\n",
    "X = X/255.0\n",
    "y = torch.Tensor([i[1] for i in training_data])\n",
    "\n",
    "VAL_PCT = 0.1\n",
    "val_size = int(len(X)*VAL_PCT)\n",
    "\n",
    "train_X = X[:-val_size]\n",
    "train_y = y[:-val_size]\n",
    "\n",
    "test_X = X[-val_size:]\n",
    "test_y = y[-val_size:]\n",
    "\n",
    "def train(net):\n",
    "    BATCH_SIZE = 100\n",
    "    EPOCHS = 3\n",
    "    for epoch in range(EPOCHS):\n",
    "        for i in tqdm(range(0, len(train_X), BATCH_SIZE)):\n",
    "            batch_X = train_X[i:i+BATCH_SIZE].view(-1,1,50,50)\n",
    "            batch_y = train_y[i:i+BATCH_SIZE]\n",
    "            batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "\n",
    "            net.zero_grad()\n",
    "            outputs = net(batch_X)\n",
    "            loss = loss_function(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        print(loss)\n",
    "\n",
    "def test(net):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for i in tqdm(range(len(test_X))):\n",
    "            real_class = torch.argmax(test_y[i]).to(device)\n",
    "            net_out = net(test_X[i].view(-1, 1, 50, 50).to(device))[0]\n",
    "\n",
    "            predicted_class = torch.argmax(net_out)\n",
    "            if predicted_class == real_class:\n",
    "                correct += 1\n",
    "            total += 1\n",
    "    print(\"Accuracy:\", round(correct/total,3))\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the above code, we can train and then test our network, but not quite to the degree that we want. Let's start by defining a fwd pass function of what happens when we pass any data\n",
    "\n",
    "The only thing we want to check for is to see if we are training the model or not. We definitely do not want to modify weights when we do our validation data, for example. \n",
    "\n",
    "For this reason, I am going to default train to be False. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/225 [00:00<?, ?it/s]\n",
      "  0%|          | 0/225 [00:00<?, ?it/s]\n",
      "  0%|          | 0/225 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc: 0.57  Loss: 0.2492\n",
      "Acc: 0.57  Loss: 0.2471\n",
      "Acc: 0.57  Loss: 0.2456\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.507, tensor(0.2514, device='cuda:0'))"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def fwd_pass(X, y, train=False):\n",
    "\n",
    "    if train: #if true\n",
    "        net.zero_grad()\n",
    "        \n",
    "    outputs = net(X)\n",
    "    matches  = [torch.argmax(i)==torch.argmax(j) for i, j in zip(outputs, y)]\n",
    "    acc = matches.count(True)/len(matches)\n",
    "    loss = loss_function(outputs, y)\n",
    "\n",
    "    if train:\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    return acc, loss\n",
    "\n",
    "net = Net().to(device)\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "loss_function = nn.MSELoss()\n",
    "\n",
    "def test(size):\n",
    "    random_start = np.random.randint(len(test_X)-size)\n",
    "    X,y = test_X[random_start:random_start+size], test_y[random_start:random_start+size]\n",
    "    with torch.no_grad():\n",
    "        val_acc, val_loss = fwd_pass(X.view(-1,1,50,50).to(device),y.to(device))\n",
    "    return val_acc, val_loss\n",
    "\n",
    "def train(net):\n",
    "    BATCH_SIZE = 100\n",
    "    EPOCHS = 3\n",
    "    for epoch in range(EPOCHS):\n",
    "        for i in tqdm(range(0, len(train_X), BATCH_SIZE)):\n",
    "            batch_X = train_X[i:i+BATCH_SIZE].view(-1,1,50,50)\n",
    "            batch_y = train_y[i:i+BATCH_SIZE]\n",
    "            batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "\n",
    "            acc, loss = fwd_pass(batch_X, batch_y, train=True)\n",
    "\n",
    "            print(f\"Acc: {round(float(acc),2)}  Loss: {round(float(loss),4)}\")\n",
    "            #Just to show the above working, \n",
    "            #Outprinting this all out every step is going to get a bit absurd, every batch in each epoch, that is alot \n",
    "            if i == 5:\n",
    "                break\n",
    "            break\n",
    "\n",
    "train(net)\n",
    "test(size=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SInce outprinting is crazy. Instead, let's just save it to some log file then we can print all onto the file\n",
    "\n",
    "time.time() The time() function returns the number of seconds passed since epoch. For Unix system, January 1, 1970, 00:00:00 at UTC is epoch (the point where time begins).Dec 28, 2018. Using this to name our file.\n",
    "\n",
    "In addition, now we want to test and validate very 50 steps during the training by using the test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|ââââââââââ| 225/225 [00:23<00:00,  9.71it/s]\n",
      "100%|ââââââââââ| 225/225 [00:23<00:00,  9.63it/s]\n",
      "100%|ââââââââââ| 225/225 [00:22<00:00,  9.79it/s]\n",
      "100%|ââââââââââ| 225/225 [00:22<00:00,  9.79it/s]\n",
      "100%|ââââââââââ| 225/225 [00:22<00:00, 10.00it/s]\n",
      "100%|ââââââââââ| 225/225 [00:23<00:00,  9.60it/s]\n",
      "100%|ââââââââââ| 225/225 [00:24<00:00,  9.04it/s]\n",
      "100%|ââââââââââ| 225/225 [00:23<00:00,  9.42it/s]\n",
      "100%|ââââââââââ| 225/225 [00:24<00:00,  9.30it/s]\n",
      "100%|ââââââââââ| 225/225 [00:24<00:00,  9.16it/s]\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "MODEL_NAME = f\"model-{int(time.time())}\"  \n",
    "# gives a dynamic model name, to just help with things getting messy over time. \n",
    "net = Net().to(device)\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "loss_function = nn.MSELoss()\n",
    "\n",
    "def fwd_pass(X, y, train=False):\n",
    "\n",
    "    if train: #if true\n",
    "        net.zero_grad()\n",
    "        \n",
    "    outputs = net(X)\n",
    "    matches  = [torch.argmax(i)==torch.argmax(j) for i, j in zip(outputs, y)]\n",
    "    acc = matches.count(True)/len(matches)\n",
    "    loss = loss_function(outputs, y)\n",
    "\n",
    "    if train:\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    return acc, loss\n",
    "\n",
    "def test(size=''):\n",
    "    random_start = np.random.randint(len(test_X)-size)\n",
    "    X,y = test_X[random_start:random_start+size], test_y[random_start:random_start+size]\n",
    "    with torch.no_grad():\n",
    "        val_acc, val_loss = fwd_pass(X.view(-1,1,50,50).to(device),y.to(device))\n",
    "    return val_acc, val_loss\n",
    "\n",
    "def train(net):\n",
    "    BATCH_SIZE = 100\n",
    "    EPOCHS = 10\n",
    "\n",
    "    with open(\"model.log\", \"a\") as f:\n",
    "        for epoch in range(EPOCHS):\n",
    "            for i in tqdm(range(0, len(train_X), BATCH_SIZE)):\n",
    "                batch_X = train_X[i:i+BATCH_SIZE].view(-1,1,50,50)\n",
    "                batch_y = train_y[i:i+BATCH_SIZE]\n",
    "                batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "\n",
    "                acc, loss = fwd_pass(batch_X, batch_y, train=True)\n",
    "                \n",
    "                if i % 50 == 0:\n",
    "                    val_acc, val_loss = test(size=100)\n",
    "                    f.write(f\"{MODEL_NAME},{round(time.time(),3)},{round(float(acc),2)},{round(float(loss), 4)},{round(float(val_acc),2)},{round(float(val_loss),4)}\\n\")\n",
    "\n",
    "train(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start, I am just going to use Matplotlib. Later you could get fancy and use something like Dash, or even just Pygal, or something else entirely. Heck, if you want, check out tensorboardx! That's the fun thing about Pytorch. It's Pythonic and open. You can do whatever you want. No one is forcing you into anything! I am showing you one of many ways for you to do a thing.\n",
    "\n",
    "Now we could graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'MODEL_NAME' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-98a329e39e77>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mstyle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"ggplot\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mmodel_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMODEL_NAME\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mcreate_acc_loss_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'MODEL_NAME' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import style\n",
    "style.use(\"ggplot\")\n",
    "\n",
    "model_name = MODEL_NAME \n",
    "\n",
    "def create_acc_loss_graph(model_name):\n",
    "    contents = open(\"model.log\", \"r\").read().split(\"\\n\")\n",
    "\n",
    "    times = []\n",
    "    accuracies = []\n",
    "    losses = []\n",
    "    val_accs = []\n",
    "    val_losses = []\n",
    "    \n",
    "    contents = contents.remove(\" \")\n",
    "    for c in contents:\n",
    "        if model_name in c: #incase there are mutiple model in the same file\n",
    "            name, timestamp, acc, loss, val_acc, val_loss = c.split(\",\")\n",
    "\n",
    "            times.append(float(timestamp))\n",
    "            accuracies.append(float(acc))\n",
    "            losses.append(float(loss))\n",
    "            val_accs.append(float(val_acc))\n",
    "            val_losses.append(float(val_loss))\n",
    "\n",
    "\n",
    "    fig = plt.figure() #multiple figures, or if just 2 data, then normal plt.plot(x,y), plt.show()\n",
    "\n",
    "    ax1 = plt.subplot2grid((2,1), (0,0))\n",
    "    ax2 = plt.subplot2grid((2,1), (1,0), sharex=ax1)\n",
    "\n",
    "    ax1.plot(times, accuracies, label=\"acc\")\n",
    "    ax1.plot(times, val_accs, label=\"val_acc\")\n",
    "    ax1.legend(loc=2)\n",
    "    ax2.plot(times,losses, label=\"loss\")\n",
    "    ax2.plot(times,val_losses, label=\"val_loss\")\n",
    "    ax2.legend(loc=2)\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "create_acc_loss_graph(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright so clearly it was more like 4-5 epochs (each epoch is ~25 seconds atm, and our X axis is time) before things started to go sour. So now we know, at least with our current model/settings, that somewhere between 4 and 5 epochs is all we want to train for. We can see how bad things already are getting by 8-10 epochs. Validation accuracy doesn't seem to suffer much, interestingly enough, but we'd still probably want to stop far before this.\n",
    "\n",
    "Hopefully this has helped you to visualize neural networks learning, and the main stats that you want to track. Also, you can see just how well neural networks can generalize initially, but later overfit and just memorize the data later on. You can see that we're getting 100% accuracy on some of those batches, and, over time, that deviation from 100% accuracy is shrinking. If we keep training for more and more epochs, this model will eventually most likely reach a perfect 100% accuracy on in-sample data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full Code\n",
    "\n",
    "```python\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import time\n",
    "\n",
    "REBUILD_DATA = False # set to true to one once, then back to false unless you want to change something in your training data.\n",
    "\n",
    "class DogsVSCats():\n",
    "    IMG_SIZE = 50\n",
    "    CATS = \"PetImages/Cat\"\n",
    "    DOGS = \"PetImages/Dog\"\n",
    "    TESTING = \"PetImages/Testing\"\n",
    "    LABELS = {CATS: 0, DOGS: 1}\n",
    "    training_data = []\n",
    "\n",
    "    catcount = 0\n",
    "    dogcount = 0\n",
    "\n",
    "    def make_training_data(self):\n",
    "        for label in self.LABELS:\n",
    "            print(label)\n",
    "            for f in tqdm(os.listdir(label)):\n",
    "                if \"jpg\" in f:\n",
    "                    try:\n",
    "                        path = os.path.join(label, f)\n",
    "                        img = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n",
    "                        img = cv2.resize(img, (self.IMG_SIZE, self.IMG_SIZE))\n",
    "                        self.training_data.append([np.array(img), np.eye(2)[self.LABELS[label]]]) \n",
    "\n",
    "                        if label == self.CATS:\n",
    "                            self.catcount += 1\n",
    "                        elif label == self.DOGS:\n",
    "                            self.dogcount += 1\n",
    "                    except Exception as e:\n",
    "                        pass\n",
    "\n",
    "        np.random.shuffle(self.training_data)\n",
    "        np.save(\"training_data.npy\", self.training_data)\n",
    "        print('Cats:',dogsvcats.catcount)\n",
    "        print('Dogs:',dogsvcats.dogcount)\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__() \n",
    "        self.conv1 = nn.Conv2d(1, 32, 5) \n",
    "        self.conv2 = nn.Conv2d(32, 64, 5) \n",
    "        self.conv3 = nn.Conv2d(64, 128, 5)\n",
    "        self.fc1 = nn.Linear(2*2*128, 512) \n",
    "        self.fc2 = nn.Linear(512, 2) \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), (2, 2))\n",
    "        x = F.max_pool2d(F.relu(self.conv3(x)), (2, 2))\n",
    "        x = x.view(-1, 512)  \n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x) \n",
    "        return F.softmax(x, dim=1) \n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\") \n",
    "    print(\"Running on the GPU\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Running on the CPU\")\n",
    "\n",
    "if REBUILD_DATA:\n",
    "    dogsvcats = DogsVSCats()\n",
    "    dogsvcats.make_training_data()\n",
    "\n",
    "training_data = np.load(\"training_data.npy\", allow_pickle=True)\n",
    "\n",
    "X = torch.Tensor([i[0] for i in training_data]).view(-1, 50, 50)\n",
    "X = X/255.0\n",
    "y = torch.Tensor([i[1] for i in training_data])\n",
    "\n",
    "MODEL_NAME = f\"model-{int(time.time())}\"  \n",
    "net = Net().to(device)\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "loss_function = nn.MSELoss()\n",
    "\n",
    "def fwd_pass(X, y, train=False):\n",
    "\n",
    "    if train: #if true\n",
    "        net.zero_grad()\n",
    "        \n",
    "    outputs = net(X)\n",
    "    matches  = [torch.argmax(i)==torch.argmax(j) for i, j in zip(outputs, y)]\n",
    "    acc = matches.count(True)/len(matches)\n",
    "    loss = loss_function(outputs, y)\n",
    "\n",
    "    if train:\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    return acc, loss\n",
    "\n",
    "def test(size=''):\n",
    "    random_start = np.random.randint(len(test_X)-size)\n",
    "    X,y = test_X[random_start:random_start+size], test_y[random_start:random_start+size]\n",
    "    with torch.no_grad():\n",
    "        val_acc, val_loss = fwd_pass(X.view(-1,1,50,50).to(device),y.to(device))\n",
    "    return val_acc, val_loss\n",
    "\n",
    "def train(net):\n",
    "    BATCH_SIZE = 100\n",
    "    EPOCHS = 10\n",
    "\n",
    "    with open(\"model.log\", \"a\") as f:\n",
    "        for epoch in range(EPOCHS):\n",
    "            for i in tqdm(range(0, len(train_X), BATCH_SIZE)):\n",
    "                batch_X = train_X[i:i+BATCH_SIZE].view(-1,1,50,50)\n",
    "                batch_y = train_y[i:i+BATCH_SIZE]\n",
    "                batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "\n",
    "                acc, loss = fwd_pass(batch_X, batch_y, train=True)\n",
    "                \n",
    "                if i % 50 == 0:\n",
    "                    val_acc, val_loss = test(size=100)\n",
    "                    f.write(f\"{MODEL_NAME},{round(time.time(),3)},{round(float(acc),2)},{round(float(loss), 4)},{round(float(val_acc),2)},{round(float(val_loss),4)}\\n\")\n",
    "\n",
    "train(net)\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
